{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f4cf619-367d-4671-a577-06e077c8d3c1",
   "metadata": {},
   "source": [
    "## Direct online store update with spark\n",
    "In this section we will follow the paradigm of direct updates to both the online and the offline store from the ETL/transformation pipeline. In this demo we will emit the offline store updates.\n",
    "\n",
    "1. Use `pyspark` to process the stream\n",
    "2. Use `spark-redis` to push the feature vectors directly into Redis\n",
    "3. Use `redis-py` to access the online store data\n",
    "\n",
    "<img src=\"./images/diagram1.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97585f76-2052-4573-b405-ecc4f58bf117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53f7e0c8-a4fa-4700-859c-f7fc1c0e317f",
   "metadata": {},
   "source": [
    "Creat a spark session and create a schema to parse incoming messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ca43ac3-f174-4944-b48c-effd1068903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "schema = StructType([ \n",
    "StructField(\"event_timestamp\", LongType(), False), \n",
    "StructField(\"driver_id\", LongType(), False), \n",
    "StructField(\"conv_rate\", FloatType(), False), \n",
    "StructField(\"acc_rate\", FloatType(), False),\n",
    "StructField(\"avg_daily_trips\", IntegerType(), False),\n",
    "StructField(\"created\", LongType(), False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bd0d21e-b66f-4648-8fd8-a540f6ca1a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_stream = spark \\\n",
    "    .readStream \\\n",
    "    .schema(schema) \\\n",
    "    .parquet(\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b28732f-18c2-4625-a3d0-de7205a86004",
   "metadata": {},
   "source": [
    "Define the stream process. Save only the latest update per each entity on the online store.\n",
    "\n",
    "\n",
    "<img src=\"./images/saving_schema.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb1a5654-7a17-47b6-8afd-f08a77e0cb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/10 22:11:56 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5b57ec11-9c0c-43a6-8c6c-82517cf34df4. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/06/10 22:11:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "def process_stream(df, _):\n",
    "    for driver_id in df.select('driver_id').distinct():\n",
    "        driver_df = df.filter(df.driver_id == driver_id)\n",
    "        driver_df = driver_df.filter(driver_df.event_timestamp == driver_df.agg({\"event_timestamp\": \"max\"}).collect()[0][0])\n",
    "        driver_df.write.format(\"org.apache.spark.sql.redis\").option(\"table\", \"drivers\").option(\"key.column\", \"driver_id\").save(mode=\"append\")\n",
    "        # Here you can also write to your offline store\n",
    "        \n",
    "query = parsed_stream.writeStream.outputMode(\"append\").foreachBatch(process_stream).start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f5eaed-3853-47de-8d6c-7aa267dea53e",
   "metadata": {},
   "source": [
    "### Getting features from Redis\n",
    "After we have uploaded our latest feature values into Redis, we need to pull them out and use them with the application model.\n",
    "In this demo we will use `redis-py` to create the connection and pull the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "898c8269-afcd-492d-89aa-4db580267a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import os\n",
    "r = redis.Redis(host = os.environ['REDIS_HOST'], port = os.environ['REDIS_PORT'], password=os.environ['REDIS_PASSWORD'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab1ba36-f0ed-4302-a847-bb3c4e5f68a9",
   "metadata": {},
   "source": [
    "Define the retrival function. For each entity in the table, fetch its required features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d90da74-f137-4659-bc6e-5fc13830bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_online_feature(r: redis.Redis, table:str, entities, features):\n",
    "    pipe = r.pipeline(transaction=False)\n",
    "    for entity_id in entities:\n",
    "        pipe.hmget(f'{table}:{entity_id}', *features)\n",
    "    return pipe.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79152b54-25c4-4ad0-ba47-385ccac70cc1",
   "metadata": {},
   "source": [
    "Check network latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6455ebfd-3116-4d2c-80a7-07d0c8f92e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.75 ms, sys: 2.73 ms, total: 7.48 ms\n",
      "Wall time: 21.3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "r.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067dc45e-6201-44ed-a1db-d076581097c6",
   "metadata": {},
   "source": [
    "Retrive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0a0f9be-3235-4cef-926d-eef7a1b22a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.73 ms, sys: 1.61 ms, total: 4.34 ms\n",
      "Wall time: 20.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[b'303', b'0.5645701', b'0.6563286'],\n",
       " [b'787', b'0.38299075', b'0.181249'],\n",
       " [b'596', b'0.49776912', b'0.25255522'],\n",
       " [b'832', b'0.40254834', b'0.20854962'],\n",
       " [b'606', b'0.0778933', b'0.66774946']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_online_feature(r, \"drivers\", [1001, 1002, 1003, 1004, 1005], [\"avg_daily_trips\", \"conv_rate\", \"acc_rate\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d7dd9-7860-4223-85a6-2c3cb81f0371",
   "metadata": {},
   "source": [
    "### Summary\n",
    "We saw the usage of `spark-redis` in order to get streaming data into redis and get your online features ready to use. \n",
    "\n",
    "Target persona: MLOps engineers/Data engineers/Platform architects.\n",
    "Consider using this approach if:\n",
    "1. You are building your own feature store product.\n",
    "    1. In control of your data path\n",
    "    1. In control on your data modeling/access patterns/data serilization or compression.\n",
    "\n",
    "\n",
    "Moreover, you can use `spark-redis` in order to digest Redis streams and use Redis as your stream message borker (not in this demo scope).\n",
    "\n",
    "<img src=\"./images/redis-streams.png\" width=\"800\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
